# slide_4PixelNN
Simple NN - with Back Propagation - Swedish (highschool)

Gymnasiekurs för 1:or (fast med matte lite högre), i två delar. Jag tar exemplet om en enda neuron, utan aktiverinsgsfunktion,
och visar hur även något sådant enkelt kan åstadkomma åtminstone något liknaknde y = kx + m, i form av anpassning
á la linjär regression. Denna kurs använder *inga* automatiseringar som PyTorch eller TensorFLow etc. Det är implementering ner på grundnivå, och allt är urbenat, med de enkalste av medel. Inga klasser, bara def's

I uppföljningen provar vi en fyravärdig vektor som input i syfte att träna ett nätverk att skilja mellan de enklaste
mönster, typ vertikal, horisontell, diagonal eller enfärgad. Här introduceras också linjär algebra på ett mjukt sätt.

Försök görs att förklara "back propagation" för elever som ännu inte vet vad derivering innebär, än mindre 
kedjeregeln (matte 4).

Dock kanske inte helt fulltäckande, och borde nog presenteras tillsammans med undervisning (tror jag föreläste 5 timmar)

Fungerande kod och presentaion, till vad som skulle kunna vara en månadslång kurs med övningar och frågor. 
Välkomna att använda. Hör av er ifall frågor osv :)

Hälsningar

Ulf

PS O ja just det ... Klicka på NNforts_4px.html, som har länkar till slides samt första enkla neuron-exemplet. Annars finns även ipynb-filerna var för sig
